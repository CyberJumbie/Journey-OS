# S-F-22-1: Critic Agent Service

**Epic:** E-22 (Critic Agent & Review Router)
**Feature:** F-10
**Sprint:** 13
**Lane:** faculty (P3)
**Size:** M

## User Story
As a **Faculty member**, I need an AI critic agent that scores generated questions on 6 quality metrics so that only high-quality items reach the review queue and low-quality items are caught early.

## Acceptance Criteria
- [ ] Critic agent uses Claude Opus for 6-metric scoring
- [ ] Metrics: clinical accuracy, pedagogical alignment, distractor quality, stem clarity, Bloom fidelity, bias detection
- [ ] Each metric scored 1-5 with justification text
- [ ] Composite score computed as weighted average (weights configurable per institution)
- [ ] Critic returns `CriticResult` with per-metric scores, justifications, and composite
- [ ] Toulmin argumentation structure stored: claim, evidence, warrant, backing, qualifier, rebuttal
- [ ] Provenance chain: links critic output to source concept, SLO, and generation node
- [ ] Custom error classes: `CriticAgentError`, `ScoringTimeoutError`
- [ ] 10-14 API tests: all 6 metrics, composite calculation, weight config, Toulmin structure, timeout handling
- [ ] TypeScript strict, named exports only

## Implementation Layers
| Layer | Package | Files |
|-------|---------|-------|
| Types | packages/types | `src/review/critic.types.ts`, `src/review/toulmin.types.ts` |
| Service | apps/server | `src/services/review/critic-agent.service.ts`, `src/services/review/scoring.service.ts` |
| Prompt | apps/server | `src/services/review/prompts/critic-prompt.builder.ts` |
| Errors | apps/server | `src/errors/critic.errors.ts` |
| Tests | apps/server | `src/tests/review/critic-agent.test.ts`, `src/tests/review/scoring.test.ts` |

## Dependencies
- **Blocks:** S-F-22-2
- **Blocked by:** S-F-21-1 (validation engine exists)
- **Cross-epic:** S-F-21-1 (Sprint 12 validation)

## Notes
- Claude Opus selected for critic role due to superior reasoning capability
- Toulmin model: Claim (the score), Evidence (specific item features), Warrant (why feature implies quality), Backing (NBME guidelines), Qualifier (confidence), Rebuttal (counter-arguments)
- Critic prompt should be few-shot with golden examples per metric
- Rate limiting: critic calls are expensive â€” batch where possible, cache results
- Metric weights default: clinical accuracy 0.25, pedagogical alignment 0.20, distractor quality 0.20, stem clarity 0.15, Bloom fidelity 0.10, bias detection 0.10
