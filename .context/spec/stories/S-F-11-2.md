# S-F-11-2: Voyage AI Embedding Integration

**Epic:** E-11 (Content Processing Pipeline)
**Feature:** F-05
**Sprint:** 4
**Lane:** faculty (P3)
**Size:** M

## User Story
As a **Faculty member**, I need content chunks embedded with Voyage AI so that semantic search and concept extraction can operate on vector representations.

## Acceptance Criteria
- [ ] VoyageEmbedService wrapping Voyage AI API (voyage-3-large model)
- [ ] Batch embedding: process up to 128 chunks per API call
- [ ] Output: 1024-dimensional float32 vectors per chunk
- [ ] Rate limiting: respect Voyage AI rate limits with exponential backoff
- [ ] Embedding vectors stored in Supabase pgvector column
- [ ] Error handling: retry transient API errors, surface permanent failures
- [ ] Input validation: reject empty strings, truncate chunks exceeding model max tokens
- [ ] 8-10 API tests covering batch embedding, rate limiting, error handling, dimension validation

## Implementation Layers
| Layer | Package | Files |
|-------|---------|-------|
| Types | packages/types | `src/content/embedding.types.ts` |
| Service | apps/server | `src/services/voyage.service.ts` |
| Config | apps/server | `src/config/voyage.config.ts` |
| Tests | apps/server | `src/tests/voyage.service.test.ts` |

## Dependencies
- **Blocks:** none
- **Blocked by:** S-F-11-1 (pipeline orchestration calls embedding service)
- **Cross-epic:** none

## Notes
- All embeddings in Journey OS are 1024-dim (architecture rule) -- enforce in type and runtime validation
- Voyage AI voyage-3-large supports up to 16,000 tokens input; chunks are 800 tokens so well within limits
- Batch processing amortizes API overhead: group chunks by content_id for efficient processing
- Store API key in environment variable VOYAGE_API_KEY, never hardcoded
- VoyageEmbedService implements `IEmbedService` interface for testability and future provider swaps
- pgvector column type: `vector(1024)` with HNSW index for cosine similarity search
