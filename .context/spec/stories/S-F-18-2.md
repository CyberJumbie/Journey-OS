# S-F-18-2: Generation Nodes

**Epic:** E-18 (LangGraph.js Generation Pipeline)
**Feature:** F-09
**Sprint:** 6
**Lane:** faculty (P3)
**Size:** L

## User Story
As a **Faculty member**, I need LLM-powered generation nodes for vignette, stem, distractors, and rationale so that the pipeline produces clinically accurate, pedagogically sound USMLE-style questions.

## Acceptance Criteria
- [ ] InitNode: validates input parameters, fetches SLO and concept context from Neo4j
- [ ] ContextFetchNode: retrieves source material embeddings (1024-dim Voyage AI) for RAG context
- [ ] VignetteGenNode: generates clinical vignette with patient presentation, history, findings
- [ ] StemGenNode: generates question stem aligned to vignette and target SLO
- [ ] DistractorGenNode: generates 3 plausible wrong answers + 1 correct answer with clinical reasoning
- [ ] RationaleGenNode: generates educational rationale explaining correct answer and why distractors are wrong
- [ ] MetadataTagNode: auto-tags question with USMLE system, discipline, difficulty, Bloom's level
- [ ] Each node uses structured output (JSON mode) with Zod validation on LLM response
- [ ] Prompt templates externalized in config, not hardcoded in node implementations
- [ ] Streaming support: each node yields partial results via LangGraph streaming
- [ ] 15-18 API tests: per-node output validation, prompt injection guards, malformed LLM response handling, context window overflow
- [ ] Named exports only, TypeScript strict

## Implementation Layers
| Layer | Package | Files |
|-------|---------|-------|
| Types | packages/types | `src/generation/nodes.types.ts`, `src/generation/prompts.types.ts` |
| Nodes | apps/server | `src/services/generation/nodes/init.node.ts`, `src/services/generation/nodes/context-fetch.node.ts`, `src/services/generation/nodes/vignette-gen.node.ts`, `src/services/generation/nodes/stem-gen.node.ts`, `src/services/generation/nodes/distractor-gen.node.ts`, `src/services/generation/nodes/rationale-gen.node.ts`, `src/services/generation/nodes/metadata-tag.node.ts` |
| Prompts | apps/server | `src/services/generation/prompts/generation-prompts.ts` |
| Tests | apps/server | `src/tests/generation/nodes/vignette-gen.test.ts`, `src/tests/generation/nodes/stem-gen.test.ts`, `src/tests/generation/nodes/distractor-gen.test.ts`, `src/tests/generation/nodes/rationale-gen.test.ts` |

## Dependencies
- **Blocks:** S-F-18-3
- **Blocked by:** S-F-18-1 (pipeline scaffold)
- **Cross-epic:** none

## Notes
- All LLM calls use `ChatOpenAI` or `ChatAnthropic` via LangChain.js abstraction â€” provider-agnostic
- Structured output via `.withStructuredOutput(zodSchema)` on the LLM instance
- Vignette generation target: <3s streaming time
- DistractorGenNode generates all 4 options in a single call with reasoning chains
- MetadataTagNode uses embedding similarity against USMLE framework nodes in Neo4j
- Prompt templates should include few-shot examples for each node type
- Guard against prompt injection: sanitize user-provided context before inclusion in prompts
